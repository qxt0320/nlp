{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d049ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_corpus, load_stopwords, save_corpus\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27826bfc",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a1502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2025-05-20 11:03:51,556 - DEBUG - Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2025-05-20 11:03:51,560 - DEBUG - Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.345 seconds.\n",
      "2025-05-20 11:03:52,905 - DEBUG - Loading model cost 1.345 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2025-05-20 11:03:52,907 - DEBUG - Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "stopwords = load_stopwords(\"stopwords.txt\")\n",
    "TRAIN_PATH = \"train2.txt\"\n",
    "TEST_PATH = \"test2.txt\"\n",
    "train_data = load_corpus(TRAIN_PATH)\n",
    "test_data = load_corpus(TEST_PATH)\n",
    "df_train = pd.DataFrame(train_data, columns=[\"text\", \"label\"])\n",
    "df_test = pd.DataFrame(test_data, columns=[\"text\", \"label\"])\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "MODEL_PATH = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "bert = BertModel.from_pretrained(MODEL_PATH, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "bert = bert.to(device)\n",
    "learning_rate = 1e-3\n",
    "input_size = 768\n",
    "num_epoches = 1\n",
    "batch_size = 100\n",
    "decay_rate = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df[\"text\"].tolist()\n",
    "        self.label = df[\"label\"].tolist()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "train_dataset = MyDataset(df_train)\n",
    "test_dataset = MyDataset(df_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return self.sigmoid(out)\n",
    "net = SentimentClassifier(input_size).to(device)\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    y_pred, y_true, y_prob = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            tokens = tokenizer(texts, \n",
    "                             padding=True, \n",
    "                             truncation=True, \n",
    "                             max_length=512,\n",
    "                             return_tensors=\"pt\")\n",
    "            \n",
    "            input_ids = tokens[\"input_ids\"].to(device)\n",
    "            attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            outputs = bert(input_ids, attention_mask=attention_mask)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            probas = model(cls_embedding).view(-1)\n",
    "            preds = (probas > 0.5).long()\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_prob.extend(probas.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad7e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [10/77], Loss: 0.6750\n",
      "Epoch [1/1], Step [20/77], Loss: 0.5619\n",
      "Epoch [1/1], Step [30/77], Loss: 0.4902\n",
      "Epoch [1/1], Step [40/77], Loss: 0.4489\n",
      "Epoch [1/1], Step [50/77], Loss: 0.4196\n",
      "Epoch [1/1], Step [60/77], Loss: 0.4030\n",
      "Epoch [1/1], Step [70/77], Loss: 0.3580\n",
      "\n",
      "Epoch 1 评估结果:\n",
      "模型已保存到 ./model/bert_dnn_1.model\n",
      "\n",
      "训练时间: 40.63秒\n"
     ]
    }
   ],
   "source": [
    "# 训练函数\n",
    "def train(model, train_loader, test_loader):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "    for epoch in range(num_epoches):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            tokens = tokenizer(texts, \n",
    "                             padding=True, \n",
    "                             truncation=True, \n",
    "                             max_length=512,\n",
    "                             return_tensors=\"pt\")\n",
    "            \n",
    "            input_ids = tokens[\"input_ids\"].to(device)\n",
    "            attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                bert_outputs = bert(input_ids, attention_mask=attention_mask)\n",
    "                cls_embedding = bert_outputs.last_hidden_state[:, 0, :]\n",
    "            outputs = model(cls_embedding).view(-1)\n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epoches}], Step [{i+1}/{len(train_loader)}], Loss: {total_loss/10:.4f}\")\n",
    "                total_loss = 0\n",
    "        # 学习率衰减\n",
    "        scheduler.step()\n",
    "        # 评估\n",
    "        print(f\"\\nEpoch {epoch+1} 评估结果:\")\n",
    "        evaluate(model, test_loader)\n",
    "        # 保存模型\n",
    "        model_path = f\"./model/bert_dnn_{epoch+1}.model\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"模型已保存到 {model_path}\\n\")\n",
    "# 训练模型\n",
    "start_time=time.time()\n",
    "train(net, train_loader, test_loader)\n",
    "time=time.time()-start_time\n",
    "print(f\"训练时间: {time:.2f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f95343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "def predict_sentiment(texts, model_path=\"./model/bert_dnn_8.model\"):\n",
    "    # 加载模型\n",
    "    model = SentimentClassifier(input_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize输入\n",
    "    tokens = tokenizer(texts, \n",
    "                      padding=True, \n",
    "                      truncation=True, \n",
    "                      max_length=512,\n",
    "                      return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        bert_outputs = bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = bert_outputs.last_hidden_state[:, 0, :]\n",
    "        predictions = model(cls_embedding)\n",
    "    \n",
    "    # 返回结果\n",
    "    results = []\n",
    "    for text, prob in zip(texts, predictions.cpu().numpy()):\n",
    "        sentiment = \"正面\" if prob > 0.5 else \"负面\"\n",
    "        results.append(f\"文本: {text}\\n情感: {sentiment} (置信度: {prob[0]:.4f})\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0caacdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: 剧情老套，充满套路和硬凹的感动.\n",
      "情感: 负面 (置信度: 0.0660)\n",
      "\n",
      "文本: 食物份量十足，性价比超高，吃得很满足!\n",
      "情感: 正面 (置信度: 0.9951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1834/604079376.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"剧情老套，充满套路和硬凹的感动.\",\n",
    "    \"食物份量十足，性价比超高，吃得很满足!\",\n",
    "]\n",
    "\n",
    "print(predict_sentiment(sample_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1b881",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b591c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\86135\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '在一个没有网络的世界里 。 所 以 有 网 络 时 必 须 找 一 个 地 方 。 不 找 寻 一 个 真 实 的 自 我 ， 而 是 看 到 一 个 有 意 思 的 世 界 ， 而 不 是 只 是 感 官 上 的 想 法 。 如 果 这 个 世 界 是 虚 空 的 ， 那 么 大 人 物 就 是 现 实 的 ， 要 找 一 个 像 样 的 孩 子 、 长 辈 去 教 育 ， 你 就 是 虚 空 了 。 假 如 他 认 为 自 己 真 的 虚 空 了 ， 我 想 他 也 不 会 这 么 做 。 说 到 这 里 的 时 候 ， 王 小 波 和 张 嘉 佳 ， 他 们 都 还 有 一 些 不 成 熟 的 想 法 ， 但 他 们 在 《 小 波 沉 思 录 》 中 说 ， 虚 空 的 理 由 不 就 是 虚 空 吗 ？ 不 就 是 说 我 不 存 在 什 么 东 西 吗 ？ 王 小 波 说 ， 自 己 可 能 有 些 感 伤 ， 甚 至 有 些 悲 痛 ， 毕 竟 自 己 不 是 从 小 就 接 受 这 种 教 育 ， 我 只 是 喜 欢 这 个 世 界 ， 我 觉 得 在 现 实 工 作 中 我 们 能 够 做 的 事 情 真 的 非 常 多 。 如 果 这 个 世 界 上 存 在 什 么 东 西 ， 这 个 世 界 就 真 的 没 有 意 义 。 如 果 世 界 上 的 意 义 在 于 虚 空 ， 那 么 大 人 物 就 是 现 实 的 ， 要 找 一 个 像 样 的 孩 子 、 长 辈 ， 你 就 是 现 实 的 、 有 意 思 的 ， 要 去 教 育 孩 子 和 长 辈 。 有 孩 子 的 王 小 波 说 ， 在 我 们 的 世 界 观 中 ， 那 些 没 有 网 络 的 世 界 是 虚 空 的 ， 没 有 家 的 时 候 是 虚 空 的 。 在 有 孩 子 的 世 界 里 ， 孩 子 真 的 能 够 有 空 吗 ？ 王 小 波 解 释 说 ， 没 有 孩 子 没 法 有 精 神 世 界 ， 没 有 精 神 世 界 有 精 神 世 界 ， 没 有 精 神 世 界 。 这 时 候 家 庭 就 不 是 用 来 教 授 知 识 的 地 方 ， 不 仅 仅 是 学 分 的 问 题'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./gpt2-chinese-cluecorpussmall\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-chinese-cluecorpussmall\")\n",
    "text_generator = TextGenerationPipeline(model, tokenizer)   \n",
    "result = text_generator(\"在一个没有网络的世界里\", max_length=500, do_sample=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
